{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIA Pacemaker - Python 과정\n",
    "\n",
    "# Python 실습예제 #4\n",
    "\n",
    "기초부터 문제까지 차근차근 Build-up\n",
    "\n",
    "- requests\n",
    "- beautifulsoup\n",
    "- selenium\n",
    "\n",
    "\n",
    "---\n",
    "### 필수 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지들을 설치합니다. 최초 1회만 실행하면 됩니다.\n",
    "\n",
    "%pip install requests beautifulsoup4 selenium pandas openpyxl\n",
    "\n",
    "# 설치가 끝나면 kernel을 \"Restart\" 합니다.\n",
    "\n",
    "# 크롬드라이버는 https://wikidocs.net/137919 를 참조하여 설치합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 연습 1 - requests (html 문서 가져오기)\n",
    "\n",
    "사용 방법  \n",
    "\n",
    "1. requests 패키지 import\n",
    "2. 가져올 url을 문자열로 입력하기\n",
    "3. requests의 get 함수를 이용해 url로부터 html이 담긴 자료 받아오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html> <html lang=\"ko\"> <head> <meta charset=\"utf-8\"> <meta name=\"referrer\" content=\"always\">  <meta name=\"format-detection\" content=\"telephone=no,address=no,email=no\"> <meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0,maximum-scale=2.0\"> <meta property=\"og:title\" content=\"금메달 : 네이버 뉴스검색\"/> <meta property=\"og:image\" content=\"https://ssl.pstatic.net/sstatic/search/common/og_v3.png\"> <meta property=\"og:description\" content=\"'금메달'의 네이버 뉴스검색 결과입니다.\"> <meta name=\"description\" lang=\"ko\" content=\"'금메달'의 네이버 뉴스검색 결과입니다.\"> <title>금메달 : 네이버 뉴스검색</title> <link rel=\"shortcut icon\" href=\"https://ssl.pstatic.net/sstatic/search/favicon/favicon_191118_pc.ico\">  <link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"https://ssl.pstatic.net/sstatic/search/opensearch-description.https.xml\" title=\"Naver\" /><link rel=\"stylesheet\" type=\"text/css\" href=\"https://ssl.pstatic.net/sstatic/search/pc/css/search1_220217.css\"> <link rel=\"stylesheet\" type=\"text/css\" href=\"htt\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "import requests\n",
    "\n",
    "\n",
    "#2\n",
    "url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query=금메달'\n",
    "\n",
    "\n",
    "#3\n",
    "response = requests.get(url)\n",
    "\n",
    "html_text = response.text\n",
    "\n",
    "\n",
    "# 일부분을 출력해보자\n",
    "print(html_text[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 연습 2 - beautifulsoup (긴 html 문서에서 원하는 것을 가져온다.)\n",
    "\n",
    "사용방법\n",
    "1. beautifulsoup 등 패키지 import\n",
    "2. url로부터 html 가져오기 (연습1과 동일)\n",
    "3. html을 정리된 형태로 변환시키기 (beautifulsoup 객체)\n",
    "4. select_one 함수 사용해서 뉴스 제목 가져오기\n",
    "\n",
    "\n",
    "html에서 가져와야 하는 요소 확인하기:  \n",
    ". 크롬 브라우저에서 원하는 페이지로 이동 후, F12를 눌러 개발자 도구를 실행합니다.  \n",
    ". 그 후, 원하는 정보가 담긴 부분에 커서를 올리면 해당 class의 이름을 볼 수 있습니다. ex) a.news_tit 등..\n",
    "\n",
    "\n",
    "<img src='exercise04_image01.png' width=\"800px\" height=\"450px\"></img>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[올림픽] 김연아 금메달 '뺏은' 소트니코바 \"안나 챔피언클럽 온 것 환영\"\n",
      "http://yna.kr/AKR20220218070700007?did=1195m\n"
     ]
    }
   ],
   "source": [
    "#1 \n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "\n",
    "#2\n",
    "url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query=금메달'\n",
    "response = requests.get(url)\n",
    "html_text = response.text\n",
    "\n",
    "\n",
    "#3\n",
    "soup = bs(html_text, 'html.parser')\n",
    "\n",
    "\n",
    "#4\n",
    "a_tag = soup.select_one('a.news_tit')\n",
    "title = a_tag.get_text()\n",
    "href = a_tag.attrs['href']\n",
    "\n",
    "# 위에서 'news_tit' 클래스를 가진 <a ...> 태그 중 하나를 가져왔다.\n",
    "# 텍스트와 링크를 출력해보자\n",
    "print(title)\n",
    "print(href)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 문제 1\n",
    "#### 기사 제목 웹스크래핑하기 \n",
    "\n",
    "1. '쇼트트랙'을 검색한 네이버 뉴스 페이지를 가져오고 (html) \n",
    "2. 이 페이지에 있는 뉴스 제목과 링크 주소를 전부 출력하세요.\n",
    "\n",
    "힌트: \n",
    "1. 네이버 뉴스 검색 페이지 주소: https://search.naver.com/search.naver?where=news&sm=tab_jum&query=원하는검색어\n",
    "2. 뉴스 링크에 해당하는 태그.클래스:  a.news_tit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 쇼트트랙 빅토르안·김선태 한국 온다…SNS에 남긴 소감은 링크: https://www.chosun.com/sports/sports_special/2022/02/18/5XZQP7MBTVDGBL7Q2DHPMIHPV4/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "中서 韓 쇼트트랙 선수 '반칙왕' 묘사한 영화 개봉…IOC 고발돼 링크: http://yna.kr/AKR20220218015700371?did=1195m\n",
      "[베이징올림픽]'세계 최강' 쇼트트랙 대표팀..밝게 웃으며 '금의환향' 링크: http://starin.edaily.co.kr/news/newspath.asp?newsid=01400566632231832\n",
      "'세계최강' 증명한 쇼트트랙 대표팀, 팬들 환호 속 금의환향 링크: http://www.newsis.com/view/?id=NISX20220218_0001765051&cID=10511&pID=10500\n",
      "쇼트트랙 2관왕 런쯔웨이 \"한국 지도자 만나 강해졌다\" 링크: https://hankookilbo.com/News/Read/A2022021809170003645?did=NA\n",
      "한국 오는 김선태·안현수…中 한국 쇼트트랙 DNA 심기 ‘절반의 성공’ 링크: https://www.seoul.co.kr/news/newsView.php?id=20220218500119&wlog_tag3=naver\n",
      "세계 최강 韓 쇼트트랙 금의환향…'금2·은3' 빛나는 활약 링크: http://www.busan.com/view/busan/view.php?code=2022021820321740944\n",
      "'최강 재확인' 쇼트트랙 대표팀 귀국 링크: https://www.ytn.co.kr/_ln/0107_202202182130081524\n",
      "한국 쇼트트랙 선수는 반칙왕?… 중국 영화 ‘명예훼손’ 논란 링크: http://www.segye.com/content/html/2022/02/18/20220218503719.html?OutUrl=naver\n",
      "\"이번엔 오륜기 하트 세리머니\"…'흥 넘치는' 남자 쇼트트랙 대표팀 링크: http://news.mk.co.kr/newsRead.php?no=154657&year=2022\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "\n",
    "url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query=쇼트트랙'\n",
    "\n",
    "response = requests.get(url)\n",
    "html_text = response.text\n",
    "\n",
    "soup = bs(html_text, 'html.parser')\n",
    "\n",
    "a_tags = soup.select('a.news_tit')\n",
    "\n",
    "for a in a_tags:\n",
    "  href = a.attrs['href']\n",
    "  title = a.get_text()\n",
    "  print(title, '링크:', href)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 2\n",
    "#### 원하는 지역의 기온 가져오기\n",
    "\n",
    "1. input을 통해 지역을 입력받고,\n",
    "2. 해당 지역의 기온 정보 출력하기\n",
    "\n",
    "힌트:\n",
    "1. 특정지역의 네이버 날씨 검색 주소: https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=OO날씨 \n",
    "2. 기온이 들어있는 태그.클래스:  크롬 개발자 모드를 사용하여 직접 알아내 보세요.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울 기온:  현재 온도-3° \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "\n",
    "location = input('지역을 입력하세요: ')\n",
    "url = f'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query={location}날씨'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "html_text = response.text\n",
    "\n",
    "\n",
    "soup = bs(html_text, 'html.parser')\n",
    "\n",
    "temperature = soup.select_one('div.temperature_text').get_text()\n",
    "\n",
    "print(location, '기온:', temperature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 연습 3 - selenium으로 화면 조작하기\n",
    "\n",
    "1. selenium 패키지 import\n",
    "2. 크롬 드라이버 실행\n",
    "3. 크롬 드라이버로 url 실행\n",
    "4. 키보드를 이용한 검색 (자동)\n",
    "5. 마우스 클릭 (자동)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "#2\n",
    "s = Service('/Users/brix/.chromedriver')  # 내 PC에 저장한 chromedriver의 경로를 줄 것\n",
    "driver = webdriver.Chrome(service=s)\n",
    "# 크롬드라이버 사용법 사이트 ( https://jaehyongschool.tistory.com/16 )\n",
    "\n",
    "\n",
    "#3 \n",
    "driver.get('https://www.naver.com/')\n",
    "time.sleep(2)   # 2초간 대기\n",
    "\n",
    "\n",
    "#4\n",
    "# 검색어 창을 찾아 search_box 변수에 저장 - css_selector 이용\n",
    "search_box = driver.find_element(By.CSS_SELECTOR, 'input#query.input_text')\n",
    "\n",
    "# 검색어 창에 텍스트 입력 (키보드 입력)\n",
    "search_box.send_keys('대한민국 금메달')\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "time.sleep(2)   # 2초간 대기\n",
    "\n",
    "\n",
    "#5 \n",
    "# 뉴스 탭 찾아서 변수에 저장 - css_selector 이용\n",
    "news_link = driver.find_element(By.CSS_SELECTOR, '#lnb > div.lnb_group > div > ul > li:nth-child(8) > a')\n",
    "\n",
    "# 클릭하기\n",
    "news_link.click()\n",
    "time.sleep(2)   # 2초간 대기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 연습 4 - 네이버뉴스 댓글 수집해서 액셀로 저장하기\n",
    "\n",
    "원본 출처: https://wikidocs.net/151506  \n",
    "\n",
    "\n",
    "1. selenium, beautifulsoup, pandas 등 필요 패키지 import\n",
    "2. selenium과 크롬 드라이버 이용해 사이트 방문 (네이버뉴스 기사 페이지의 댓글보기 상태)\n",
    "3. 더보기 버튼 더이상 안나올 때까지 클릭\n",
    "4. beautifulsoup 이용해 HTML 요소 모두 파싱\n",
    "5. pandas 데이터 프레임 생성\n",
    "6. 엑셀로 저장\n",
    "\n",
    "\n",
    "<img src='exercise04_image02.png' width=\"800px\" height=\"450px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료!!\n"
     ]
    }
   ],
   "source": [
    "#1 \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "#2 \n",
    "# 크롬 드라이버로 해당 url에 접속\n",
    "s = Service('/Users/brix/.chromedriver')  # 내 PC에 저장한 chromedriver의 경로를 줄 것\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = 'https://n.news.naver.com/article/comment/092/0002248059'\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "#3\n",
    "# 더보기가 안뜰 때 까지 계속 클릭 (모든 댓글의 html을 얻기 위함)\n",
    "while True:\n",
    "    \n",
    "# 예외처리 구문 - 더보기 광클하다가 없어서 에러 뜨면 while문을 나감(break)\n",
    "  try:\n",
    "    more = driver.find_element(By.CSS_SELECTOR, 'a.u_cbox_btn_more')\n",
    "    more.click()\n",
    "    time.sleep(0.2)\n",
    "\n",
    "  except:\n",
    "    break\n",
    "\n",
    "\n",
    "#4\n",
    "# 본격적인 크롤링 타임\n",
    "\n",
    "# selenium으로 페이지 전체의 html 문서 받기\n",
    "html = driver.page_source\n",
    "\n",
    "# 위에서 받은 html 문서를 bs4 패키지로 parsing\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 1) 작성자\n",
    "nicknames = soup.select('span.u_cbox_nick')\n",
    "list_nicknames = [nickname.text for nickname in nicknames]\n",
    "\n",
    "# 2) 댓글 시간\n",
    "datetimes = soup.select('span.u_cbox_date')\n",
    "list_datetimes = [datetime.text for datetime in datetimes]\n",
    "\n",
    "# 3) 댓글 내용\n",
    "contents = soup.select('span.u_cbox_contents') \n",
    "list_contents = [content.text for content in contents]\n",
    "\n",
    "# 4)작성자, 댓글 시간, 내용을 셋트로 취합\n",
    "list_sum = list(zip(list_nicknames,list_datetimes,list_contents))\n",
    "\n",
    "# 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "#5\n",
    "# 테이블의 첫줄에 들어갈 컬럼명\n",
    "col = ['작성자', '시간', '내용']\n",
    "\n",
    "# pandas 데이터 프레임 형태로 가공\n",
    "df = pd.DataFrame(list_sum, columns=col)\n",
    "\n",
    "\n",
    "#6\n",
    "# 데이터 프레임을 엑셀로 저장 (파일명은 'news.xlsx', 시트명은 '뉴스 댓글')\n",
    "df.to_excel('news.xlsx', sheet_name='뉴스 댓글')\n",
    "\n",
    "print('저장 완료!!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 문제 3\n",
    "#### 구글 학술검색에서 \"deep learning construction industry\" 검색하여 서지정보 가져오고, 엑셀로 저장하기\n",
    "\n",
    "1. selenium을 이용해 구글 학술검색으로 이동\n",
    "2. 검색어 창을 찾아서 \"deep learning construction industry\" 검색\n",
    "3. 검색 결과 페이지에서 \"인용\" 링크 모두 찾기\n",
    "4. 각 항목에 대해:  \n",
    "  링크 클릭  \n",
    "  나타난 팝업 요소에서 MLA 서지정보 저장\n",
    "5. pandas를 이용해 엑셀로 저장\n",
    "\n",
    "힌트: \n",
    "- 구글 검색창 찾기:  driver.find_element(By.NAME, 'q')\n",
    "- 인용 버튼 element:  driver.find_element(By.CSS_SELECTOR, '#gs_res_ccl_mid > div > ... > ... ... ')  # 크롬 개발자 모드를 통해 알아내보자\n",
    "- element에서 텍스트 가져오기:  element.get_attribute(\"innerHTML\")\n",
    "- 주의: 팝업된 것은 다음 \"인용\" 버튼 클릭하기 전에 닫아주자. (가려지면 링크 클릭 안됨)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 개의 항목에 대한 링크 확보됨\n",
      "Akinosho, Taofeek D., et al. \"Deep learning in the construction industry: A review of present status and future innovations.\" Journal of Building Engineering 32 (2020): 101827.\n",
      "Alkhaddar, Rafid, et al. \"Deep learning approach's effectiveness on sustainability improvement in the UK construction industry.\" Management of Environmental Quality: An International Journal (2012).\n",
      "Fang, Qi, et al. \"A deep learning-based method for detecting non-certified work on construction sites.\" Advanced Engineering Informatics 35 (2018): 56-68.\n",
      "Xu, Yayin, et al. \"Machine learning in construction: From shallow to deep learning.\" Developments in the Built Environment 6 (2021): 100045.\n",
      "Delgado, Manuel Davila, Olugbenga O. Akinade, and Ashraf A. Ahmed. \"Deep learning in the construction industry: A review of present status and.\" Journal of Building Engineering 32 (2020): 101827.\n",
      "Zhong, Botao, et al. \"Convolutional neural network: Deep learning-based classification of building quality problems.\" Advanced Engineering Informatics 40 (2019): 46-57.\n",
      "Hou, Lei, et al. \"Deep learning-based applications for safety management in the AEC industry: A review.\" Applied Sciences 11.2 (2021): 821.\n",
      "Khallaf, Rana, and Mohamed Khallaf. \"Classification and analysis of deep learning applications in construction: A systematic literature review.\" Automation in Construction 129 (2021): 103760.\n",
      "Jacobsen, Emil L., and Jochen Teizer. \"Deep Learning in Construction: Review of Applications and Potential Avenues.\" Journal of Computing in Civil Engineering 36.2 (2022): 03121001.\n",
      "Lomio, Francesco, et al. \"Classification of building information model (BIM) structures with deep learning.\" 2018 7th European Workshop on Visual Information Processing (EUVIP). IEEE, 2018.\n",
      "저장 완료!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "#1\n",
    "s = Service('/Users/brix/.chromedriver')\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# 구글 학술검색으로 이동\n",
    "driver.get('https://scholar.google.com/')\n",
    "time.sleep(2)   # 2초간 대기\n",
    "\n",
    "\n",
    "#2\n",
    "# 검색창\n",
    "search_box = driver.find_element(By.NAME, 'q')\n",
    "\n",
    "# 검색어 창에 텍스트 입력 (키보드 입력)\n",
    "search_box.send_keys('deep learning construction industry')\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "time.sleep(2)   # 2초간 대기\n",
    "\n",
    "\n",
    "#3\n",
    "# \"인용\" 링크 모두 가져오기\n",
    "citation_buttons = driver.find_elements(By.CSS_SELECTOR, '#gs_res_ccl_mid > div > div.gs_ri > div.gs_fl > a.gs_or_cit.gs_or_btn.gs_nph > span')\n",
    "print(len(citation_buttons), '개의 항목에 대한 링크 확보됨')\n",
    "\n",
    "\n",
    "#4\n",
    "citations = []\n",
    "\n",
    "for btn in citation_buttons:\n",
    "  btn.click()\n",
    "  time.sleep(0.4)\n",
    "  \n",
    "  cit = driver.find_element(By.CSS_SELECTOR, '#gs_citt > table > tbody > tr:nth-child(1) > td > div')\n",
    "  cit_text = cit.get_attribute('innerHTML').replace('<i>', '').replace('</i>', '')\n",
    "  citations.append(cit_text)\n",
    "  \n",
    "  x_btn = driver.find_element(By.CSS_SELECTOR, '#gs_cit-x > span.gs_ico')\n",
    "  x_btn.click()\n",
    "  time.sleep(0.1)\n",
    "\n",
    "# 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "print(*citations, sep='\\n')\n",
    "\n",
    "\n",
    "#5\n",
    "# 테이블의 첫줄에 들어갈 컬럼명\n",
    "col = ['서지정보']\n",
    "\n",
    "# pandas 데이터 프레임 형태로 가공\n",
    "df = pd.DataFrame(citations, columns=col)\n",
    "\n",
    "# 데이터 프레임을 엑셀로 저장 (파일명은 'citations.xlsx', 시트명은 'citations')\n",
    "df.to_excel('citations.xlsx', sheet_name='citations')\n",
    "\n",
    "\n",
    "print('저장 완료!!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피아스페이스(주) All Rights Reserved.  \n",
    "\n",
    "https://www.pia-space.net"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
